{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e476ff80",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Optuna\n",
    "\n",
    "Welcome to the Optuna-based hyperparameter optimization tutorial! In this interactive notebook, you will explore world of hyperparameter tuning for a Convolutional Neural Network (CNN) specifically aimed at image classification using the CIFAR-10 dataset. Hyperparameter optimization is pivotal in enhancing model performance, making your models more accurate and efficient.\n",
    "\n",
    "Optuna, a robust and versatile library, plays a central role in automating and streamlining this process. It empowers you to navigate through complex hyperparameter spaces with ease. In this tutorial, you will engage with Optuna's core functionalities, and you'll also have the opportunity to construct a flexible CNN architecture. This adaptable design is essential for understanding how models can be fine-tuned effortlessly to suit various hyperparameter configurations.\n",
    "\n",
    "Throughout this session, you will:\n",
    "- Learn how to set up and execute an Optuna study, incorporating all essential elements required for effective hyperparameter optimization.\n",
    "- Perform a thorough analysis of the results to evaluate how different hyperparameters influence model performance, gaining insights into their practical impact.\n",
    "\n",
    "Additionally, this tutorial includes an optional section where you will compare two prevalent methods of hyperparameter optimization: Optuna's default sampling method (Tree-structured Parzen Estimator, or TPE) and the traditional Grid Search method. This comparison will not only highlight the strengths of Optuna but also provide a clearer perspective on how it can outperform conventional optimization techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amitvermaknw/Desktop/app/ML/deep-learning-with-pytorch/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "import helper\n",
    "\n",
    "helper.set_seed(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872b364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad4877",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization for CNNs on CIFAR-10\n",
    "\n",
    "In this section, you explore the vital task of finding the optimal hyperparameters for a Convolutional Neural Network (CNN) tailored to the CIFAR-10 dataset. \n",
    "Utilizing Optuna, a sophisticated framework for hyperparameter optimization, your goal is to streamline and automate the process, ensuring efficiency and effectiveness. \n",
    "The selection of hyperparameters is notably intensive computationally and depends on various factors including the architecture of the model, the dataset characteristics, and the specific training processes involved. These elements, collectively and individually, have significant impacts on the performance outcomes of the model.\n",
    "\n",
    "### Defining a Flexible CNN Architecture\n",
    "\n",
    "The model architecture here is deliberately designed to be flexible, accommodating variability in its layers which is pivotal for adapting to different hyperparameter configurations suggested by Optuna during optimization trials.  \n",
    "The architecture is defined in a modular manner, allowing for easy adjustments and experimentation with different layer configurations, activation functions, and other hyperparameters. \n",
    "\n",
    "`FlexibleCNN` is a class that encapsulates the architecture of the CNN model:\n",
    "\n",
    "* **`__init__`**: The constructor initializes the model's feature extraction layers.\n",
    ">    * It constructs a series of convolutional blocks based on the `n_layers` parameter. Each block is a sequence of `nn.Conv2d`, `nn.ReLU`, and `nn.MaxPool2d`.\n",
    ">    * The `in_channels` for each block is set to the `out_channels` of the preceding block to ensure a seamless data flow.\n",
    ">    * All blocks are combined into a single `nn.Sequential` module assigned to the `.features` attribute, which handles feature extraction.\n",
    ">    * The classifier, `.classifier`, is initially set to `None` and will be constructed dynamically later.\n",
    " * **`_create_classifier`**: This helper method dynamically builds the classifier part of the network.\n",
    ">    * It's called during the first forward pass once the input size for the linear layers is known.\n",
    " * **`forward`**: This method defines the forward pass of the model.\n",
    ">    * The input `x` first passes through the `.features` layers.\n",
    ">    * The output from the feature extractor is flattened to determine the input size for the classifier.\n",
    ">    * If the `.classifier` has not been created yet, it calls `_create_classifier` to build it on the fly.\n",
    ">    * Finally, the flattened data is passed through the `.classifier` to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83d5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible Convolutional Neural Network with a dynamically created classifier.\n",
    "\n",
    "    This CNN's architecture is defined by the provided hyperparameters,\n",
    "    allowing for a variable number of convolutional layers. The classifier\n",
    "    (fully connected layers) is constructed during the first forward pass\n",
    "    to adapt to the output size of the convolutional feature extractor.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, n_filters, kernel_sizes, dropout_rate, fc_size):\n",
    "        \"\"\"\n",
    "        Initializes the feature extraction part of the CNN.\n",
    "\n",
    "        Args:\n",
    "            n_layers: The number of convolutional blocks to create.\n",
    "            n_filters: A list of integers specifying the number of output\n",
    "                       filters for each convolutional block.\n",
    "            kernel_sizes: A list of integers specifying the kernel size for\n",
    "                          each convolutional layer.\n",
    "            dropout_rate: The dropout probability to be used in the classifier.\n",
    "            fc_size: The number of neurons in the hidden fully connected layer.\n",
    "        \"\"\"\n",
    "        super(FlexibleCNN, self).__init__()\n",
    "\n",
    "        #Initialize an empty list to hold the convolution blocks\n",
    "        blocks = []\n",
    "        #Set the initial number of input channels for RGB images\n",
    "        in_channels = 3\n",
    "\n",
    "        #Loop to construct each convolutional block\n",
    "        for i in range(n_layers):\n",
    "            #Get the parameters for current convolution layer\n",
    "            out_channels = n_filters[i]\n",
    "            kernel_size = kernel_sizes[i]\n",
    "\n",
    "            #Calculate padding to maintain the input spatial dimension('same' padding)\n",
    "            padding = (kernel_size-1) // 2\n",
    "\n",
    "            #Define a block as a sequence of conv, ReLU and MaxPool. layers\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "\n",
    "            #Add the newly created block to the list\n",
    "            blocks.append(block)\n",
    "\n",
    "            #Update the number of input channels for the next block\n",
    "            in_channels = out_channels\n",
    "\n",
    "        #Combine all blocks inot a single feature extractor module\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "\n",
    "        #Store hyperparameters needed for building the classifier later\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fc_size = fc_size\n",
    "\n",
    "        #The classifier will be initialized dynamically in the forward pass\n",
    "        self.classifier = None\n",
    "\n",
    "    def _create_classifier(self, flattened_size, device):\n",
    "        \"\"\"\n",
    "        Dynamically creates and initializes the classifier part of the network.\n",
    "\n",
    "        This helper method is called during the first forward pass to build the\n",
    "        fully connected layers based on the feature map size from the\n",
    "        convolutional base.\n",
    "\n",
    "        Args:\n",
    "            flattened_size: The number of input features for the first linear\n",
    "                            layer, determined from the flattened feature map.\n",
    "            device: The device to which the new classifier layers should be moved.\n",
    "        \"\"\"\n",
    "\n",
    "        #Define the classifier's architecture\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(flattened_size, self.fc_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(self.fc_size, 100)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            The output logits from the classifier.\n",
    "        \"\"\"\n",
    "        #Get the device of the input tensor to ensure consistency\n",
    "        device = x.device\n",
    "\n",
    "        #Pass the input through the feature extraction layer\n",
    "        x=self.features(x)\n",
    "\n",
    "        #Flatten the feature map to prepare it for the fully connected layers\n",
    "        flattened = torch.flatten(x, 1)\n",
    "        flattened_size = flattened.size(1)\n",
    "\n",
    "        #If the classifier has not been created yet, initialize it\n",
    "        if self.classifier is None:\n",
    "            self._create_classifier(flattened_size, device)\n",
    "\n",
    "        #pass the flattened feature through the classifier to get the final output\n",
    "        return self.classifier(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182fe0c",
   "metadata": {},
   "source": [
    "## Defining the Optuna Objective Function\n",
    "\n",
    "The objective function is the core of the hyperparameter optimization process, being the function that Optuna will repeatedly call to evaluate different hyperparameter configurations.\n",
    "This function encapsulates the entire training and evaluation process, including the definition of the CNN model architecture, the optimizer, the data loaders, the training loop, and the evaluation metrics.\n",
    "Within this function, you define the search space for hyperparameters using `trial.suggest_*` methods, which allow Optuna to sample hyperparameters from a defined range or set of values. \n",
    "For a full list of available `suggest_*` methods, you can refer to the [Optuna documentation](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html).\n",
    "\n",
    "*The objective function is designed to return a single scalar value, which represents the performance of the model for the given hyperparameters*. \n",
    "In your case, the aim to maximize the accuracy of the model on the validation set, which is computed using the `evaluate_accuracy` function.\n",
    "\n",
    "**Dynamic Layer Initialization**: A noteworthy addition to this objective function is the initialization step involving a dummy input. Because the `FlexibleCNN` creates its classifier layers dynamically during the first forward pass, these parameters do not exist immediately after the model is instantiated.\n",
    "- **Why use a dummy input?** Passing data through the model forces it to calculate the flattened feature size and build the classifier layers. You must do this *before* defining the optimizer so that `model.parameters()` includes the classifier weights. Otherwise, the optimizer would only track the feature extractor, leaving the classifier untrained.\n",
    ">\n",
    "- **Why these dimensions?** The tensor `torch.randn(1, 3, 32, 32)` is used to mimic the structure of the CIFAR-10 dataset. It represents a single image (batch size of 1) with 3 color channels (RGB) and a resolution of `32x32` pixels.\n",
    "\n",
    "Observe that some hyperparameters are defined as fixed values, such as the number of epochs, the batch size, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ca065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, device):\n",
    "    \"\"\"\n",
    "    Defines the objective function for hyperparameter optimization using Optuna.\n",
    "\n",
    "    For each trial, this function samples a set of hyperparameters,\n",
    "    constructs a model, trains it for a fixed number of epochs, evaluates\n",
    "    its performance on a validation set, and returns the accuracy. Optuna\n",
    "    uses the returned accuracy to guide its search for the best\n",
    "    hyperparameter combination.\n",
    "\n",
    "    Args:\n",
    "        trial: An Optuna `Trial` object, used to sample hyperparameters.\n",
    "        device: The device ('cpu' or 'cuda') for model training and evaluation.\n",
    "\n",
    "    Returns:\n",
    "        The validation accuracy of the trained model as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    #Sample hyperparameters for the feature extractor using the Optuna trial\n",
    "    n_layers = trial.suggest_init(\"n_layer\", 1, 3)\n",
    "    n_filters=[trial.suggest_init(f\"n_filter_{i}\", 16, 128) for i in range(n_layers)]\n",
    "    kernal_sizes = [trial.suggest_categorical(f\"kernel_size_{i}\", [3,5]) for i in range(n_layers)]\n",
    "\n",
    "    #Sample hyperparameters for the classifier\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    fc_size = trial.suggest_int(\"fc_size\", 64, 256)\n",
    "\n",
    "    #Instantiate the model with the sampled hyperparameters\n",
    "    model = FlexibleCNN(n_layers, n_filters, kernal_sizes, dropout_rate, fc_size).to(device)\n",
    "\n",
    "    #Initialize the dynamic classifier layer bt passing a dummy input through the model\n",
    "    #This ensure all parameters are instantiated before the optimizer is defined\n",
    "    dummy_input = torch.rand(1,3,32, 32).to(device)\n",
    "    model(dummy_input)\n",
    "\n",
    "    #Define fixed training parameters: lr, loss function and optimizer\n",
    "    learning_rate = 0.001\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "    #Define fixed daata loading parameters and crete data loader\n",
    "    batch_size=128\n",
    "    train_loader, val_loader = helper.get_dataset_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    #Define the fixed number of epochs for training\n",
    "    n_epochs = 10\n",
    "\n",
    "    #Train model using a helper function\n",
    "    helper.train_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fcn=loss_fcn,\n",
    "        train_dataloader=train_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    #Evaluate the trained model's accuracy on the validaation set\n",
    "    accuracy = helper.evaluate_accuracy(model, val_loader, device)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fd771",
   "metadata": {},
   "source": [
    "## Running the Optuna Study\n",
    "\n",
    "Once that the objective function is defined, an Optuna study is created to manage the hyperparameter optimization process.\n",
    "The study is responsible for running the objective function multiple times with different hyperparameter configurations, allowing Optuna to explore the search space and find the best hyperparameters.\n",
    "In this case, your goal is to **maximize the accuracy** of the CNN model on the CIFAR-10 dataset, this is why we use `direction='maximize'` when creating the study.\n",
    "The `optimize` method of the study is called to start the optimization process, which will run the objective function for a defined number of trials.\n",
    "\n",
    "A lambda function is used to pass the device to the objective function, allowing the model to be trained on the specified device\n",
    "*Note*: you can also pass other parameters to the objective function using the lambda function, if needed.\n",
    "\n",
    "**NOTE:** the code below will take about 8 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a study object and optimie the objective function\n",
    "study = optuna.create_study(direction='maximize') #The goal in this case is to maximize accuracy\n",
    "\n",
    "#Start the optimization process \n",
    "n_trials = 20\n",
    "study.optimize(lambda trial: objective(trial, device), n_trials=n_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
