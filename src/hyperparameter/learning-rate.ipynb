{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990c7fd4",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning: Learning Rate and Metrics\n",
    "\n",
    "Welcome to this first exploration of hyperparameter tuning using the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset!  This dataset consists on 60,000 32x32 color images in 10 classes (6,000 images per class). Here are the classes in the dataset, as well as 10 random images from each: \n",
    "\n",
    "![](./nb_image/cifar10.png)\n",
    "\n",
    "In this notebook, you'll focus specifically on the learning rate, an essential hyperparameter that dictates the pace at which a model learns during training. You'll work with a simple convolutional neural network (CNN) and observe how changes in hyperparameters affect the model's outcomes.\n",
    "\n",
    "This lab will cover the following:\n",
    "\n",
    "* Examining the effects of different **learning rates** on model performance.\n",
    "\n",
    "* Introducing and using additional metrics like **precision, recall, and F1 score** for a more complete evaluation.\n",
    "\n",
    "* Exploring the effect of **batch size** on different metrics using an imbalanced dataset in an optional section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c572907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "\n",
    "import helper\n",
    "\n",
    "helper.set_seed(42)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b919b",
   "metadata": {},
   "source": [
    "## Learning Rate Optimization on CIFAR-10\n",
    "\n",
    "You will now explore how the learning rate affects the performance of a simple convolutional neural network (CNN) on the CIFAR-10 dataset.\n",
    "For that you will train a simple CNN model with different learning rates and observe the validation accuracy to understand the sensitivity of the model to this hyperparameter.\n",
    "\n",
    "The code below sets up the necessary functions for training the model and evaluating its performance:\n",
    "\n",
    "- `SimpleCNN`: defines a small convolutional neural network architecture.\n",
    "\n",
    "- `evaluate_accuracy`: computes the accuracy on a validation dataset.\n",
    "\n",
    "These functions encapsulate the key elements required for running the hyperparameter optimization experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70988116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A simple Convolutional Neural Network (CNN) architecture.\n",
    "\n",
    "    This class defines a two-layer CNN with max pooling, dropout, and\n",
    "    fully connected layers, suitable for basic image classification tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the layers of the neural network.\"\"\"\n",
    "\n",
    "        #Initialize the parent nn.Module class\n",
    "        super().__init__()\n",
    "\n",
    "        #First convolution layer (3 input channels, 16 output channels , 3X3 kernel)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        #Second con layer\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        #Max pooling layer with 2X2 windows and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #First fully connected liner layer\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 64)\n",
    "\n",
    "        #Second fully connected lauer serving as a output layer\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "        #Dropout layer for regularization\n",
    "        self.droput = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output logits from the network.\n",
    "        \"\"\"\n",
    "\n",
    "        #Apply first convolution, ReLU activation and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        #Apply second convolution, ReLU activation, and max poling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        #Flatten the feature maps for the fully connected layers\n",
    "        x = x.view(-1, 32*8*8)\n",
    "\n",
    "        #Apply first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        #Apply dropout for regulatization\n",
    "        x = self.droput(x)\n",
    "\n",
    "        #Apply the final outputlayer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
